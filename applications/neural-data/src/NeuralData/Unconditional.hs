{-# LANGUAGE Arrows,TypeApplications,DeriveGeneric #-}

module NeuralData.Unconditional where
--    (
--    -- * Initialization
--      Initialization
--          ( RandomInitialization
--          , DataInitialization
--          , PartitionedDataInitialization )
--    , initializeMixtureLikelihood
--    -- * Fitting
--    , Optimization
--        ( StochasticGradientDescent
--        , ExpectationMaximization
--        , Alternating
--        , Hybrid
--        , Hybrid2 )
--    , fitMixtureLikelihood
--    , shotgunFitMixtureLikelihood
--    , LikelihoodBounds (LikelihoodBounds)
--    -- ** Stimulus Independent
--    , shotgunFitMixture
--    , shotgunEMFitMixture
--    -- * Analyses
--    , analyzePopulationCurves
--    , preferredStimulusHistogram
--    , precisionsHistogram
--    , gainsHistograms
--    -- ** Cross-Validation
--    , CrossValidationStats (CrossValidationStats)
--    , crossValidateMixtureLikelihood
--    , crossValidateEmpiricalCovariances
--    -- ** IO
--    , runPopulationParameterAnalyses
--    -- * Miscellaneous
--    , kFoldDataset
--    , kFoldConditionalDataset
--    , multipartitionList
--    , getMixtureLikelihood
--    , strengthenMixtureLikelihood
--    , dataCovarianceFunction
--    ) where


--- Imports ---


-- Goal --

import Goal.Core
import Goal.Geometry
import Goal.Probability

import Foreign.Storable
import qualified Goal.Core.Vector.Storable as S
import qualified Goal.Core.Vector.Boxed as B
import qualified Goal.Core.Vector.Generic as G

import NeuralData
import Data.String

import qualified Data.List as L
import qualified Data.Map as M
import Data.Tuple


--- IO ---



----- Stimulus Independent ---
--
--
--initializeMixture
--    :: forall r k n . (KnownNat k, KnownNat n)
--    => Natural # Dirichlet (n+1) -- ^ Initial Mixture Weights Distribution
--    -> [Response k]
--    -> Random r (Natural # Mixture (Neurons k) n)
--{-# INLINE initializeMixture #-}
--initializeMixture rmxs zs = do
--    mxs <- samplePoint rmxs
--    let gns0 = transition $ sufficientStatisticT zs
--    gnss' <- S.replicateM $ Point <$> S.replicateM (uniformR (-0.01,0.01))
--    let gnss = S.map (gns0 <+>) gnss'
--        nctgl = toNatural . Point @ Source $ S.tail mxs
--    return $ joinMixture gnss nctgl
--
--fitMixture
--    :: forall r k n . (KnownNat k, KnownNat n)
--    => Double -- ^ Learning Rate
--    -> Double -- ^ Weight decay rate
--    -> Int -- ^ Batch size
--    -> Int -- ^ Number of epochs
--    -> Natural # Dirichlet (n+1) -- ^ Initial Mixture Weights Distribution
--    -> [Response k]
--    -> Random r [Natural # Mixture (Neurons k) n]
--{-# INLINE fitMixture #-}
--fitMixture eps dcy nbtch nepchs rmxs zs = do
--    mmdl0 <- initializeMixture rmxs zs
--    let grdcrc = loopCircuit' mmdl0 $ proc (zs',mmdl) -> do
--            let dmmdl = vanillaGradient $ stochasticMixtureDifferential zs' mmdl
--                dcymmdl = weightDecay dcy mmdl
--            gradientCircuit eps defaultAdamPursuit -< joinTangentPair dcymmdl dmmdl
--    streamCircuit grdcrc . take nepchs . breakEvery nbtch $ cycle zs
--
--emFitMixture
--    :: forall r k n . (KnownNat k, KnownNat n)
--    => Int -- ^ Number of epochs
--    -> Natural # Dirichlet (n+1) -- ^ Initial Mixture Weights Distribution
--    -> [Response k]
--    -> Random r [Natural # Mixture (Neurons k) n]
--{-# INLINE emFitMixture #-}
--emFitMixture nepchs rmxs zs = do
--    mmdl0 <- initializeMixture rmxs zs
--    return . take nepchs $ iterate (mixtureExpectationMaximization zs) mmdl0
--
--shotgunFitMixture
--    :: (KnownNat k, KnownNat m)
--    => Int -- ^ Number of parallel fits
--    -> Double -- ^ Learning rate
--    -> Double -- ^ Weight Decay
--    -> Int -- ^ Batch size
--    -> Int -- ^ Number of epochs
--    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
--    -> [Response k] -- ^ Training data
--    -> Random r ([CrossEntropyDescentStats],Int,Natural # Mixture (Neurons k) m)
--{-# INLINE shotgunFitMixture #-}
--shotgunFitMixture nshtgn eps dcy nbtch nepchs drch zs = do
--    mmdlss <- replicateM nshtgn $ fitMixture eps dcy nbtch nepchs drch zs
--    let cost hrm = negate . average $ logMixtureDensity hrm <$> zs
--    return $ filterShotgun mmdlss cost
--
--shotgunEMFitMixture
--    :: (KnownNat k, KnownNat m)
--    => Int -- ^ Number of parallel fits
--    -> Int -- ^ Number of epochs
--    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
--    -> [Response k] -- ^ Training data
--    -> Random r ([CrossEntropyDescentStats],Int,Natural # Mixture (Neurons k) m)
--{-# INLINE shotgunEMFitMixture #-}
--shotgunEMFitMixture nshtgn nepchs drch zs = do
--    mmdlss <- replicateM nshtgn $ emFitMixture nepchs drch zs
--    let cost hrm = negate . average $ logMixtureDensity hrm <$> zs
--    return $ filterShotgun mmdlss cost
--
--
----- CSV ---
--
--
--data PreferredStimuli = PreferredStimuli
--    { psBinCentre :: Double
--    , preferredStimulusCount :: Int
--    , preferredStimulusDensity :: Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord PreferredStimuli where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered PreferredStimuli where
--    headerOrder = goalCSVOrder
--
--data Precisions = Precisions
--    { pBinCentre :: Double
--    , precisionCount :: Int
--    , precisionDensity :: Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord Precisions where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered Precisions where
--    headerOrder = goalCSVOrder
--
--data ParameterDistributionFit = ParameterDistributionFit
--    { parameterValue :: Double
--    , densityFit :: Double
--    , trueDensity :: Maybe Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord ParameterDistributionFit where
--    toNamedRecord = goalCSVNamer
--instance DefaultOrdered ParameterDistributionFit where
--    headerOrder = goalCSVOrder
--
--data Gains = Gains
--    { gBinCentre :: Double
--    , gainCount :: Int
--    , gainDensity :: Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord Gains where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered Gains where
--    headerOrder = goalCSVOrder
--
--data DataDependenceStats = DataDependenceStats
--    { trainingSetSize :: Int
--    , dataDependenceAverage :: Double
--    , dataDependenceMinimum :: Double
--    , dataDependenceMaximum :: Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord DataDependenceStats where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered DataDependenceStats where
--    headerOrder = goalCSVOrder
--
--data LikelihoodBounds = LikelihoodBounds
--    { independentUpperBound :: Double
--    , trueLowerBound :: Maybe Double }
--    deriving (Show,Generic)
--
--instance ToNamedRecord LikelihoodBounds where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered LikelihoodBounds where
--    headerOrder = goalCSVOrder
--
--data CrossValidationStats = CrossValidationStats
--    { numberOfMixtureComponents :: Int
--    , cvNegativeLogLikelihood :: Double
--    , informationGain :: Double }
--    deriving (Show,Generic)
--
----data CrossValidationStats = CrossValidationStats
----    { numberOfMixtureComponents :: Int
----    , modelNegativeLogLikelihood :: Double
----    , modelCovarianceDivergence :: Double
----    , empiricalCovarianceDivergence :: Double }
----    deriving (Show,Generic)
--
--instance ToNamedRecord CrossValidationStats where
--    toNamedRecord = goalCSVNamer
--
--instance DefaultOrdered CrossValidationStats where
--    headerOrder = goalCSVOrder
--
--data TuningCurves k = TuningCurves
--    { tcStimulus :: Double
--    , tuningCurves :: S.Vector k Double }
--    deriving (Show,Generic)
--
--instance KnownNat k => ToNamedRecord (TuningCurves k) where
--    toNamedRecord (TuningCurves stm tcs) =
--        let stmrc = "Stimulus" .= stm
--            tcrcs = countRecords "Tuning Curve" tcs
--         in namedRecord $ stmrc : tcrcs
--
--instance KnownNat k => DefaultOrdered (TuningCurves k) where
--    headerOrder _ =
--        orderedHeader $ "Stimulus" : countHeaders "Tuning Curve" (Proxy @ k)
--
--data GainProfiles m = GainProfiles
--    { gpStimulus :: Double
--    , gainProfiles :: S.Vector m Double }
--    deriving (Show,Generic)
--
--instance KnownNat m => ToNamedRecord (GainProfiles m) where
--    toNamedRecord (GainProfiles stm gns) =
--        let stmrc = "Stimulus" .= stm
--            gnrcs = countRecords "Gain" gns
--         in namedRecord $ stmrc : gnrcs
--
--instance KnownNat m => DefaultOrdered (GainProfiles m) where
--    headerOrder _ =
--        orderedHeader $ "Stimulus" : countHeaders "Gain" (Proxy @ m)
--
--data ConjugacyCurves m = ConjugacyCurves
--    { rcStimulus :: Double
--    , sumOfTuningCurves :: S.Vector m Double
--    , conjugacyCurve :: Double }
--    deriving (Show,Generic)
--
--instance KnownNat m => ToNamedRecord (ConjugacyCurves m) where
--    toNamedRecord (ConjugacyCurves stm stcs cnj) =
--        let stmrc = "Stimulus" .= stm
--            cnjrc = "Conjugacy Curve" .= cnj
--            stcrc = countRecords "Sum of Tuning Curves" stcs
--         in namedRecord $ stmrc : stcrc ++ [cnjrc]
--
--instance KnownNat m => DefaultOrdered (ConjugacyCurves m) where
--    headerOrder _ =
--         orderedHeader $ "Stimulus" : countHeaders "Sum of Tuning Curves" (Proxy @ m) ++ ["Conjugacy Curve"]
--
--data CategoryDependence m = CategoryDependence
--    { cdStimulus :: Double
--    , categoryStimulusDependence :: S.Vector m Double }
--    deriving (Show,Generic)
--
--instance KnownNat m => ToNamedRecord (CategoryDependence m) where
--    toNamedRecord (CategoryDependence stm cts) =
--        let stmrc = "Stimulus" .= stm
--            ctrcs = countRecords "Category" cts
--         in namedRecord $ stmrc : ctrcs
--
--instance KnownNat m => DefaultOrdered (CategoryDependence m) where
--    headerOrder _ = orderedHeader $ "Stimulus" : countHeaders "Category" (Proxy @ m)
--
--
----- Internal ---
--
--
--countHeaders :: KnownNat n => String -> Proxy n -> [ByteString]
--countHeaders ttl prxn = [ fromString (ttl ++ ' ' : show i) | i <- take (natValInt prxn) [(0 :: Int)..] ]
--
--countRecords
--    :: forall x n . (ToField x, KnownNat n, Storable x)
--    => String
--    -> S.Vector n x
--    -> [(ByteString,ByteString)]
--countRecords ttl = zipWith (.=) (countHeaders ttl $ Proxy @ n) . S.toList
--
---- | Returns (validation,training) pairs
--kFoldDataset :: Int -> [x] -> [([x],[x])]
--{-# INLINE kFoldDataset #-}
--kFoldDataset k xs =
--    let nvls = ceiling . (/(fromIntegral k :: Double)) . fromIntegral $ length xs
--     in L.unfoldr unfoldFun ([], breakEvery nvls xs)
--    where unfoldFun (_,[]) = Nothing
--          unfoldFun (hds,tl:tls) = Just ((tl,concat $ hds ++ tls),(tl:hds,tls))
--
--kFoldConditionalDataset
--    :: KnownNat k
--    => Int -> [(Response k, Double)] -> [([(Response k, Double)],[(Response k, Double)])]
--{-# INLINE kFoldConditionalDataset #-}
--kFoldConditionalDataset k zxs =
--    let mp = M.fromListWith (++) . zip (snd <$> zxs) $ (:[]) <$> zxs
--     in [ foldr1 folder vtzxss | vtzxss <- L.transpose . M.elems $ kFoldDataset k <$> mp ]
--    where folder (vzxs',tzxs') (vzxs,tzxs) = (vzxs' ++ vzxs,tzxs' ++ tzxs)
--
----- Graveyard ---
--
--
----multiFitMixtureLikelihood
----    :: (KnownNat k, KnownNat m)
----    => Int -- ^ Number of parallel fits
----    -> Double -- ^ Learning rate
----    -> Int -- ^ Batch size
----    -> Int -- ^ Number of epochs
----    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
----    -> Natural # LogNormal -- ^ Initial Precisions
----    -> [(Response k, Double)] -- ^ Training data
----    -> [(Response k, Double)] -- ^ Validation data
----    -> Random r ([CrossEntropyDescentStats], Int, Mean #> Natural # ConditionalMixture (Neurons k) m VonMises)
----{-# INLINE multiFitMixtureLikelihood #-}
----multiFitMixtureLikelihood npop eps nbtch nepchs drch lgnrm tzxs vzxs = do
----    let (vzs,vxs) = unzip vzxs
----    mlklss <- replicateM npop $ sgdFitMixtureLikelihood eps nbtch nepchs drch lgnrm tzxs
----    let cost = mixtureStochasticConditionalCrossEntropy vxs vzs
----        mlklcstss = do
----            mlkls <- mlklss
----            return . zip mlkls $ cost <$> mlkls
----        (nanmlklcstss,mlklcstss') = L.partition (any (\x -> isNaN x || isInfinite x) . map snd) mlklcstss
----        sgdnrms = costsToCrossEntropyDescentStats <$> L.transpose (map (map snd) mlklcstss')
----        mxmlkl = fst . L.minimumBy (comparing snd) $ last <$> mlklcstss'
----    return (sgdnrms, length nanmlklcstss, mxmlkl)
--
----mixtureLikelihoodDataDependence
----    :: forall k m r . (KnownNat k, KnownNat m)
----    => Int -- ^ Number of data sets to generate
----    -> Int -- ^ Number of shotgun fits
----    -> Double -- ^ Learning rate
----    -> Double -- ^ Weight decay
----    -> Int -- ^ Batch size
----    -> Int -- ^ Number of epochs
----    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
----    -> Natural # LogNormal -- ^ Initial Precisions
----    -> Mean #> Natural # ConditionalMixture (Neurons k) m VonMises -- ^ True Mixture Model
----    -> Sample VonMises
----    -> Int -- ^ Data set size
----    -> Random r ([DataDependenceStats], Mean #> Natural # ConditionalMixture (Neurons k) m VonMises)
----{-# INLINE mixtureLikelihoodDataDependence #-}
----mixtureLikelihoodDataDependence nsts nshtgn eps dcy nbtch nepchs drch lgnrm plkl xsmps sz = do
----    zxss <- replicateM nsts $ sampleMixtureLikelihood plkl sz
----    (_,_,qlklss) <- unzip3
----        <$> mapM (shotgunFitMixtureLikelihood nshtgn eps dcy nbtch nepchs drch lgnrm) zxss
----    let qlkls = fst . L.minimumBy (comparing snd) $ do
----            qlkls' <- qlklss
----            return (qlkls', conditionalMixtureRelativeEntropyUpperBound xsmps plkl $ last qlkls')
----    let ddsts = map (upperBoundsToCrossEntropyDescentStats sz) . L.transpose
----            $ map (map $ conditionalMixtureRelativeEntropyUpperBound xsmps plkl) qlklss
----    return (ddsts, last qlkls)
----    -- = kFoldDataset kfld zxs
----    --csts <- mapM (validateMixtureLikelihood nshtgn eps nbtch nepchs drch lgnrm) tvxzss
----    --return $ CrossValidationStats (natValInt $ Proxy @ (m+1)) (average csts) (minimum csts) (maximum csts)
----
----sampleMixtureLikelihood
----    :: (KnownNat k, KnownNat m)
----    => Mean #> Natural # ConditionalMixture (Neurons k) m VonMises
----    -> Int
----    -> Random r [(Response k, Double)]
----sampleMixtureLikelihood mlkl sz = do
----    xs <- sample sz uni
----    zs <- mapM (fmap hHead . samplePoint) $ mlkl >$>* xs
----    return $ zip zs xs
----        where uni :: Source # VonMises
----              uni = Point $ S.doubleton 0 0
--
---- | Given a list of percentages, breaks a list into sublists of approximate
---- chunks with the given percenteges. If the sum of the percentages is smaller
---- than one, elements will be dropped, and if the sum is greater than one,
---- additional percentages will be ignored.
----multipartitionList
----    :: [Double]
----    -> [x]
----    -> [[x]]
----multipartitionList wghts xs0 =
----    let nsmps = length xs0
----        stps0 = round . (fromIntegral nsmps *) <$> wghts
----     in L.unfoldr unfolder (xs0,stps0)
----    where unfolder (x:xs,stp:stps) =
----              let (hdxs,tlxs) = splitAt stp (x:xs)
----               in Just (hdxs,(tlxs,stps))
----          unfolder _ = Nothing
--
----emFitMixtureLikelihood
----    :: forall k n . (KnownNat k, KnownNat n)
----    => [(Response k,Double)]
----    -> Mean #> Natural # ConditionalMixture (Neurons k) n VonMises
----    -> [Mean #> Natural # ConditionalMixture (Neurons k) n VonMises]
----{-# INLINE emFitMixtureLikelihood #-}
----emFitMixtureLikelihood zxs =
----    iterate (mixturePopulationExpectationMaximization zxs)
----
----shotgunFitMixtureLikelihood
----    :: (KnownNat k, KnownNat m)
----    => Int -- ^ Number of steps between performance evaluation
----    -> Int -- ^ Number of parallel fits
----    -> Double -- ^ Learning rate
----    -> Double -- ^ Weight Decay
----    -> Int -- ^ Batch size
----    -> Int -- ^ Number of epochs
----    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
----    -> Natural # LogNormal -- ^ Initial Precisions
----    -> [(Response k, Double)] -- ^ Training data
----    -> Random r ([CrossEntropyDescentStats],Int,[Mean #> Natural # ConditionalMixture (Neurons k) m VonMises])
----{-# INLINE shotgunFitMixtureLikelihood #-}
----shotgunFitMixtureLikelihood skp nshtgn eps dcy nbtch nepchs drch lgnrm zxs = do
----    let (zs,xs) = unzip zxs
----    mlklss <- replicateM nshtgn $ sgdFitMixtureLikelihood eps dcy nbtch nepchs drch lgnrm zxs
----    let cost = mixtureStochasticConditionalCrossEntropy (take 1000 xs) (take 1000 zs)
----    return $ filterShotgun skp mlklss cost
----
----validateMixtureLikelihood
----    :: (KnownNat k, KnownNat m)
----    => Int -- ^ Number of parallel fits
----    -> Double -- ^ Learning rate
----    -> Double -- ^ Weight decay
----    -> Int -- ^ Batch size
----    -> Int -- ^ Number of epochs
----    -> Natural # Dirichlet (m + 1) -- ^ Initial mixture parameters
----    -> Natural # LogNormal -- ^ Initial Pre,Validation data
----    -> ([(Response k, Double)],[(Response k, Double)]) -- ^ Validation/Training data
----    -> Random r Double
----{-# INLINE validateMixtureLikelihood #-}
----validateMixtureLikelihood nshtgn eps dcy nbtch nepchs drch lgnrm (vzxs,tzxs) = do
----    (_,_,mxmlkl) <- shotgunFitMixtureLikelihood 100 nshtgn eps dcy nbtch nepchs drch lgnrm tzxs
----    let (vzs,vxs) = unzip vzxs
----    return . mixtureStochasticConditionalCrossEntropy vxs vzs $ last mxmlkl
----
--
